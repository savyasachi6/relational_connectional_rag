# RAG Platform Architectural Specification

This document provides a comprehensive architectural specification for the production-grade Retrieval-Augmented Generation (RAG) platform. It details the data flows, entity relationships, and failure-mode boundaries across the entire stack, designed for implementation by mid-to-senior backend engineers, ML engineers, and technical leads.

---

## 1. High-level end-to-end architecture

The RAG platform consists of three primary regions. First, the **Data Sources & Ingestion Pipeline** handles acquiring data from various sources (documents, code, databases, S3), parsing it, chunking it, enriching it, and storing the embeddings in Postgres via pgvector. Second, the **Query Path & Reasoning Engine** serves client applications, taking queries through a FastAPI layer, retrieving relevant context via semantic and keyword search, and orchestrating multi-agent reasoning to formulate an answer using an LLM provider. Finally, the **Observability, Evaluation, and Validation Layer** acts as a safeguard and monitoring plane. It validates answers (via Gatekeeper, Auditor, Strategist), tracks metrics, and provides tools for red-teaming the system to ensure safety, relevance, and performance.

```mermaid
graph TD
    %% External Entities
    Client[Client Applications / Users]
    S3[(Object Storage / S3)]
    LLMProvider((LLM Provider))
    
    %% RAG Backend Regions
    subgraph QueryPath [Query Path & Reasoning Engine]
        API[FastAPI Service]
        Retriever[Hybrid Retriever]
        Orchestrator[Multi-Agent Orchestrator]
    end
    
    subgraph DataSources [Data Sources & Ingestion Pipeline]
        IngestService[Ingestion Worker]
        Parser[Document Parser]
        Enricher[Metadata Enricher]
    end
    
    subgraph StorageLayer [Storage Layer]
        PG[(Postgres + pgvector)]
    end
    
    subgraph ObservabilityLayer [Observability, Evaluation, and Validation]
        Validator[Validation Pipeline<br/>Gatekeeper, Auditor, Strategist]
        Eval[Evaluation & Red-teaming]
        Dashboard[Metrics Dashboard]
    end
    
    %% Flows
    Client -->|POST /ask| API
    S3 -->|Raw Data| IngestService
    IngestService --> Parser --> Enricher
    Enricher -->|Vectorize & Store| PG
    
    API --> Orchestrator
    Orchestrator -->|Search| Retriever
    Retriever -->|Query| PG
    Orchestrator -->|Generate Answers| LLMProvider
    Orchestrator -->|Draft Answer| Validator
    Validator -.->|Check Grounding & Safety| LLMProvider
    Validator -->|Final Answer| API
    
    API -.->|Telemetry| Eval
    IngestService -.->|Telemetry| Eval
    Eval --> Dashboard
```

*Caption: Top-level component diagram showing the interaction between external clients, ingestion workflows, query/reasoning paths, and the validation/evaluation mechanisms, all backed by Postgres+pgvector and an external LLM provider.*

---

## 2. Ingestion and enrichment pipeline

The ingestion pipeline is designed to be asynchronous and highly parallelizable. It begins with document acquisition and unstructured data extraction (using tools like PyMuPDF or LlamaParse). It applies strict structure-aware chunking to preserve logical boundaries (e.g., paragraphs, tables, code blocks). The chunks then flow into the enrichment phase where LLMs generate summaries, extract keywords, and propose hypothetical questions that this chunk could answer. Following enrichment, chunks are passed through an embedding model and persisted into Postgres+pgvector, bundled with version control, access-control labels, and the generated metadata.

```mermaid
graph LR
    %% Data Sources
    RawDocs[(S3 / Blob Storage)] -->|Trigger| IngestWorker
    
    subgraph IngestionPipeline [Ingestion Pipeline]
        IngestWorker[Document Ingestion] --> Parser[Structure Extraction<br/>PyMuPDF/LlamaParse]
        
        Parser -->|Async| Chunking[Structure-Aware Chunking]
        
        Chunking -->|Parallel| Enrich1[Summary Generation]
        Chunking -->|Parallel| Enrich2[Keyword Extraction]
        Chunking -->|Parallel| Enrich3[Hypothetical Qs]
        
        Enrich1 --> Merger[Metadata Merger]
        Enrich2 --> Merger
        Enrich3 --> Merger
        
        Merger --> Vectorizer[Vectorization / LLM Embeddings]
    end
    
    Vectorizer -->|Write Chunks & Vectors| DB[(Postgres Database)]
```

*Caption: Flowchart detailing the asynchronous and parallelizable data ingestion process, from raw document extraction to metadata enrichment and vectorization.*

---

## 3. Data-model diagram (Postgres + pgvector)

The core entities within our Postgres schema include `Document`, `Chunk`, and `ChunkEmbedding`. A single document maps to multiple structured chunks. Each chunk contains raw content as well as enriched metadata (summaries, keywords, hypothetical queries), and maps 1-to-1 with a `ChunkEmbedding`, which stores the `vector(dim)` generated by the embedding model. This schema naturally handles versioning mapping documents to their iterations, and supports row-level security or simple column filtering for multitenancy/department isolation. Hybrid search is achieved by joining `pgvector` similarity operations on the `ChunkEmbedding` table with trigram or full-text search operators on the `Chunk.content` and `Chunk.keywords` columns.

```mermaid
erDiagram
    Document {
        UUID id PK
        string source_uri "URL or S3 path"
        int version "Document version iteration"
        string mime_type "e.g., application/pdf"
        jsonb metadata "Access control, author, etc."
        timestamp created_at
    }
    
    Chunk {
        UUID id PK
        UUID document_id FK
        int ordinal "Chunk position sequence"
        string heading "Section heading context"
        text content "Raw chunk text"
        text summary "LLM-generated summary"
        jsonb hypothetical_questions
        jsonb keywords "For FTS/Trigram search"
        jsonb metadata
    }
    
    ChunkEmbedding {
        UUID chunk_id PK, FK
        vector vector "pgvector column e.g. 1536 dims"
    }
    
    Document ||--o{ Chunk : "contains"
    Chunk ||--|| ChunkEmbedding : "has_embedding"
```

*Caption: ER diagram outlining the primary Postgres schema. Note the separation of raw document tracking from chunks and their specific pgvector embeddings.*

---

## 4. FastAPI + retrieval API layer

The application layer exposes endpoints such as `POST /ingest` for triggering data processing and `POST /ask` for serving queries, alongside standard `/health` endpoints. Rather than tightly coupling with vector store implementations, the FastAPI routers rely on injected abstractions (`Retriever`, `LLMClient`, `PostgresRepository`). During a `POST /ask` call, the FastAPI service delegates work to an orchestrator which prompts the retriever (accessing Postgres via injected clients), passes the retrieved context to the LLM Client, and then pushes the draft answer through a validation pipeline before returning proper HTTP responses to the client.

```mermaid
sequenceDiagram
    actor Client
    participant API as FastAPI Service
    participant Retriever as Hybrid Retriever
    participant PG as Postgres + pgvector
    participant LLM as LLM Client
    participant Validator as Validation Pipeline
    
    Client->>API: POST /ask {query, filters}
    API->>Retriever: retrieve(query, filters)
    Retriever->>PG: Semantic + FTS Query
    PG-->>Retriever: Retrieved Chunks
    Retriever-->>API: Ranked Context
    
    API->>LLM: generate_answer(query, context)
    LLM-->>API: Draft Answer
    
    API->>Validator: validate(query, answer, context)
    Validator-->>API: Validated Answer / Adjustments
    
    API-->>Client: HTTP 200 {answer, citations}
```

*Caption: Sequence diagram of the core /ask endpoint, illustrating dependency injection flow through retrieval, reasoning, and validation without tight coupling.*

---

## 5. Hybrid retrieval and reasoning engine

The reasoning layer handles exactly how user queries are translated into database lookups and actions. First, queries undergo preprocessing (expansion or decomposition). The engine performs Hybrid Search by executing semantic search (via `pgvector`) in parallel with keyword search (via Postgres Full-Text Search). The candidate datasets are merged and pushed through a cross-encoder or reciprocal rank fusion (RRF) reranker. For complex queries, a multi-step planner (built on LangGraph or a similar agent framework) can execute tool calling, loop through intermediate reasoning steps, and aggregate context until an answer is fully materialized.

```mermaid
graph TD
    Query[User Query<br/>POST /ask] --> Preprocess[Query Preprocessing & Decomposition]
    
    Preprocess -->|Sub-queries| SearchRouter{Is Simple Q?}
    
    %% Retrieval paths
    SearchRouter -->|Yes| SemSearch[Semantic Search<br/>pgvector]
    SearchRouter -->|Yes| KeySearch[Keyword Search<br/>FTS/Trigram]
    
    SemSearch --> Rerank[Merging & Reranking<br/>Cross-Encoder / RRF]
    KeySearch --> Rerank
    
    SearchRouter -->|No, Complex| Agent[Multi-Step Agent Planner]
    Agent <--> Tools[(Internal Tools / APIs)]
    Agent --> SemSearch
    
    Rerank --> Gen[LLM Answer Generation]
    Agent --> Gen
```

*Caption: Architecture of the hybrid reasoning engine, highlighting the parallel semantic/keyword retrieval, reranking, and the branch for multi-agent orchestration for complicated requests.*

---

## 6. Validation, safety, and quality layer

Prior to returning an answer to the client, the response passes through a pipeline of guardrails consisting of three main actors. The **Gatekeeper** checks if the answer conceptually aligns with and addresses the user's initial question. The **Auditor** fact-checks the answer to guarantee it is genuinely grounded within the retrieved context chunks (minimizing hallucinations). The **Strategist** evaluates coherence with business logic, brand tone, and risk compliance. A conditional router aggregates these reports; if the answer fails critical checks, the router can trigger a rewrite, append warnings, or entirely short-circuit and reject the response with a safe fallback escalation.

```mermaid
graph LR
    Draft[Draft Answer] --> Gatekeeper[Gatekeeper<br/>Relevance Check]
    Gatekeeper --> Auditor[Auditor<br/>Grounding Check]
    Auditor --> Strategist[Strategist<br/>Risk & Policy Check]
    
    Strategist --> Router{Decision Router}
    
    Router -->|Pass| Final[Final Answer]
    Router -->|Minor Issue| Rewrite[Rewrite/Warn Prompt]
    Rewrite --> Draft
    Router -->|Critical Risk| Reject[Safe Fallback / Reject Escalation]
```

*Caption: Flowchart of the validation pipeline demonstrating the Gatekeeper-Auditor-Strategist triad and the conditional safety routing acting on draft answers.*

---

## 7. Evaluation, monitoring, and red-teaming

The RAG system is continuously evaluated using a combination of qualitative judges and quantitative telemetry. An automated LLM-as-a-judge mechanism grades responses on faithfulness, depth, and relevance against an evaluation dataset. We also track quantitative metrics like Mean Reciprocal Rank (MRR), chunk retrieval precision, overall latency, and computational costs. Crucially, a parallel Red-Teaming subsystem orchestrates automated adversarial queries (testing for prompt injection, data leakage, and bias). This continuous probing guides immediate prompt or filter updates, and informs longer-term foundation model swaps.

```mermaid
graph TD
    subgraph ProductionTelemetry [Production Telemetry]
        Logs[Live Logs & Telemetry]
        GroundTruth[(Evaluation Dataset)]
    end
    
    subgraph EvaluationPipeline [Evaluation Pipeline]
        Judges[Automated LLM Judges<br/>Faithfulness/Relevance]
        Metrics[Quantitative Metrics<br/>Precision, Recall, MRR, Latency]
        Dash[Observability Dashboard]
    end
    
    subgraph RedTeamingSubsystem [Red-Teaming Subsystem]
        AttackGen[Adversarial Scenario Generator]
        Probe[Red-Team Orchestrator]
    end
    
    Logs ---> Judges & Metrics
    GroundTruth ---> Judges
    Judges ---> Dash
    Metrics ---> Dash
    
    AttackGen --> Probe
    Probe -->|Inject Malicious Queries| TargetAPI([FastAPI Target])
    TargetAPI -.->|Review Outputs| Dash
    
    Dash --> Feed[Feedback Loop<br/>Prompt Revisions / Filter Updates / Model Swaps]
```

*Caption: Component graph detailing the separation of continuous evaluation telemetry, quantitative tracking, and automated red-teaming pipelines that feed back into system hardening.*
