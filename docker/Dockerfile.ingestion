# Dockerfile for the Data Ingestion Worker
# This background worker handles restructuring raw documents, 
# structure-aware chunking, and generating metadata (summaries, QA pairs).

# Use an official Python runtime as a parent image
FROM python:3.11-slim

# Set environment variables to prevent Python from writing .pyc files
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Set the working directory within the container
WORKDIR /app

# Install system dependencies
# These may be needed for specific document parsing libraries (e.g. poppler for PDFs),
# and psycopg2 for PostgreSQL connectivity.
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    gcc \
    poppler-utils \
    tesseract-ocr \
    && rm -rf /var/lib/apt/lists/*

# Copy the requirements file into the container
# Again, assuming a generic requirements file for background workers (e.g., Celery)
COPY docker/requirements.worker.txt /app/requirements.txt

# Install Python dependencies
# In a real scenario, this installs Unstructured, LlamaParse, or similar libraries.
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code into the container
COPY src /app/src

# Set PYTHONPATH so absolute imports inside /src work correctly
ENV PYTHONPATH="/app/src:${PYTHONPATH}"

# For the blueprint, we'll just run a simple python loop to keep the container alive 
# so we can bash into it and run the ingestion worker manually if needed.
# In production, this would be a Celery worker.
CMD ["python", "-c", "import time; print('Worker alive. Waiting for jobs...'); time.sleep(86400)"]
